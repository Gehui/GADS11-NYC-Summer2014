{
 "metadata": {
  "name": "",
  "signature": "sha256:646d265f3083f83aeaf4801e44e2d0998133132848ed4d899e29d973c8f661b3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Sentence and Word Segmentation\n",
      "\n",
      "The first step in NLP is cutting text into its constituents. Namely, sentences and words. Let's see how well we can perform this task in base python.\n",
      "\n",
      "**DO NOT worry about writing efficient code.** We're just practicing NLP principles.\n",
      "\n",
      "It will useful to know the String methods! These are one of the most useful features of Python for text processing!\n",
      "\n",
      "https://docs.python.org/2/library/stdtypes.html#string-formatting-operations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Sentence segmentation\n",
      "\n",
      "Let's start with sentence segmentation. English typically end with a period, exclamation, or question mark. Let's start easy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run this cell for a HINT:\n",
      "import base64\n",
      "base64.decodestring('S2VlcCBhIGxpc3Qgb2Ygc2VudGVuY2VzLCBhbmQgYSB0ZW1wIHN0cmluZyB3aXRoIHRoZSBjdXJy\\nZW50IHNlbnRlbmNlLiBBcHBlbmQgd2hlbiB5b3UgaGl0IHRoZSByaWdodCBjaGFyYWN0ZXJz\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "'Keep a list of sentences, and a temp string with the current sentence. Append when you hit the right characters'"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# defining the input text and what the output should be.\n",
      "easy_text = \"I went to the zoo today. What do you think of that? I bet you hate it! Or maybe you don't\"\n",
      "easy_split_text = [\"I went to the zoo today.\",\n",
      "                   \"What do you think of that?\",\n",
      "                   \"I bet you hate it!\",\n",
      "                   \"Or maybe you don't\"]\n",
      "print easy_text\n",
      "print easy_split_text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "I went to the zoo today. What do you think of that? I bet you hate it! Or maybe you don't\n",
        "['I went to the zoo today.', 'What do you think of that?', 'I bet you hate it!', \"Or maybe you don't\"]\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define a function to split a string into sentences.\n",
      "# If you're familiar with regexes, feel free to use the re module\n",
      "def sentences(text):\n",
      "    '''take a string called `text` and return a list of strings, each containing a sentence'''\n",
      "    # FILL IN CODE\n",
      "    sentences = [\"sit in the classroom\",\"try to finish this lab\",\"one more hour to go\"]\n",
      "    return sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test your function by running this cell\n",
      "\n",
      "if map(str.strip, sentences(easy_text)) == easy_split_text:\n",
      "    print 'Congratulations!'\n",
      "else:\n",
      "    print 'Sorry, try again!'\n",
      "    print \n",
      "    print 'Your version:'\n",
      "    print sentences(easy_text)\n",
      "    print\n",
      "    print 'Desired output:'\n",
      "    print easy_split_text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Sorry, try again!\n",
        "\n",
        "Your version:\n",
        "['sit in the classroom', 'try to finish this lab', 'one more hour to go']\n",
        "\n",
        "Desired output:\n",
        "['I went to the zoo today.', 'What do you think of that?', 'I bet you hate it!', \"Or maybe you don't\"]\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Sentence segmentation continued\n",
      "\n",
      "What about cases where periods denote abbreviations? This time, try to do the same splits, but accommodate 'Dr.', 'Mrs.', 'Mr.', and 'Ms.'."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# defining the input text and what the output should be.\n",
      "\n",
      "med_text = \"My name is Dr. Lee. There is also a Mrs. Lee. Actually, there are tons! They're other people's wives.\"\n",
      "med_split_text = [\"My name is Dr. Lee.\",\n",
      "                  \"There is also a Mrs. Lee.\",\n",
      "                  \"Actually, there are tons!\",\n",
      "                  \"They're other people's wives.\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# modify your last sentencer to account for these new patterns.\n",
      "\n",
      "\n",
      "def sentences2(text):\n",
      "    '''take a string called `text` and return a list of strings, each containing a sentence'''\n",
      "    # FILL IN CODE\n",
      "    sentences2 = [\"sit in the classroom\",\"try to finish this lab\",\"one more hour to go\"]\n",
      "    return sentences2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test your function by running this cell\n",
      "\n",
      "if map(str.strip, sentences2(med_text)) == med_split_text:\n",
      "    print 'Congratulations!'\n",
      "else:\n",
      "    print 'Sorry, try again!'\n",
      "    print\n",
      "    print 'Your version:'\n",
      "    print sentences2(med_text)\n",
      "    print\n",
      "    print 'Desired output:'\n",
      "    print med_split_text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Sorry, try again!\n",
        "\n",
        "Your version:\n",
        "['sit in the classroom', 'try to finish this lab', 'one more hour to go']\n",
        "\n",
        "Desired output:\n",
        "['My name is Dr. Lee.', 'There is also a Mrs. Lee.', 'Actually, there are tons!', \"They're other people's wives.\"]\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Sentence segmentation continued\n",
      "\n",
      "Abbreviations like 'a.k.a.' are harder to accommodate. This one is quite challenging, so you can skip it if you want to move on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run this cell for a HINT:\n",
      "import base64\n",
      "base64.decodestring('VHJ5IGFsbG93aW5nIHRoZSBzcGxpdHMgb24gdGhlIHBlcmlvZHMsIGJ1dCB0aGVuIHJlYXR0YWNo\\naW5nIGlmIHRoZSBuZXh0IHNlbnRlbmNlIGlzIG9ubHkgb25lIGNoYXJhY3RlciBsb25n\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "'Try allowing the splits on the periods, but then reattaching if the next sentence is only one character long'"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# defining the input text and what the output should be.\n",
      "\n",
      "hard_text = \"I know an M.D., i.e. a doctor. Like Dr. Smith, a.k.a. Docsmith.\"\n",
      "hard_split_text = [\"I know an M.D., i.e. a doctor.\",\n",
      "                   \"Like Dr. Smith, a.k.a. Docsmith.\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# modify your last sentencer to account for these new patterns.\n",
      "\n",
      "def sentences3(text):\n",
      "    '''take a string called `text` and return a list of strings, each containing a sentence'''\n",
      "    # FILL IN CODE\n",
      "    sentences3 = [\"sit in the classroom\",\"try to finish this lab\",\"one more hour to go\"]\n",
      "    return sentences3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test your function by running this cell\n",
      "\n",
      "if map(str.strip, sentences3(hard_text)) == hard_split_text:\n",
      "    print 'Congratulations!'\n",
      "else:\n",
      "    print 'Sorry, try again!'\n",
      "    print\n",
      "    print 'Your version:'\n",
      "    print sentences3(hard_text)\n",
      "    print\n",
      "    print 'Desired output:'\n",
      "    print hard_split_text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Sorry, try again!\n",
        "\n",
        "Your version:\n",
        "['sit in the classroom', 'try to finish this lab', 'one more hour to go']\n",
        "\n",
        "Desired output:\n",
        "['I know an M.D., i.e. a doctor.', 'Like Dr. Smith, a.k.a. Docsmith.']\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Sentence segmentation continued\n",
      "\n",
      "Sentence segmentation is harder than it seems! Let's take a look at how a modern system does it. [NLTK](http://www.nltk.org) is the most widely-used NLP library in Python. It [relies on a statistical language model](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt) to determine when to split sentences. You'll notice that even this model can't handle our hard sentences.\n",
      "\n",
      "To get started, you have to download the right dataset. **DO NOT** download everything. It will take forever. When the download window pops up (probably behind your other windows, annoyingly) click on the 'Models' tab, choose the 'punkt' dataset, and just download that."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# download the Punkt Tokenizer Models.\n",
      "# DON'T DOWNLOAD EVERYTHING!\n",
      "# The download window will probably pop up behind your other windows.\n",
      "# uncomment the download command and comment out the print statement when you've understood these instructions.\n",
      "\n",
      "import nltk\n",
      "# nltk.download()\n",
      "print \"did you read the instructions?\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "did you read the instructions?\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk.data\n",
      "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "LookupError",
       "evalue": "\n**********************************************************************\n  Resource 'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/Users/lindahu/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-17-f461db228ee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msent_detector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/english.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Users/lindahu/anaconda/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_parser)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pickle'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0mresource_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'yaml'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/lindahu/anaconda/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/lindahu/anaconda/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/Users/lindahu/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sent_detector.sentences_from_text(easy_text)\n",
      "print sent_detector.sentences_from_text(med_text)\n",
      "print sent_detector.sentences_from_text(hard_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Word tokenization\n",
      "\n",
      "A more common task is to ignore sentences and just split text into words. We call this tokenization. Try your hand at this. This task is much easier now that you're familiar with all the string methods. right?? You should be able to write a fairly simple function that can tokenize all of our texts from before."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define our objective tokenizations. Note that we've removed some punctuation.\n",
      "easy_words = ['I', 'went', 'to', 'the', 'zoo', 'today',\n",
      "              'What', 'do', 'you', 'think', 'of', 'that',\n",
      "              'I', 'bet', 'you', 'hate', 'it',\n",
      "              'Or', 'maybe', 'you', \"don't\"]\n",
      "med_words = ['My', 'name', 'is', 'Dr', 'Lee',\n",
      "             'There', 'is', 'also', 'a', 'Mrs', 'Lee',\n",
      "             'Actually,', 'there', 'are', 'tons',\n",
      "             \"They're\", 'other', \"people's\", 'wives']\n",
      "hard_words = ['I', 'know', 'an', 'MD,', 'ie', 'a', 'doctor',\n",
      "              'Like', 'Dr', 'Smith,', 'aka', 'Docsmith']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define a function to split a string into sentences.\n",
      "# If you're familiar with regexes, feel free to use the re module\n",
      "\n",
      "def tokenizer(text):\n",
      "    '''take a string called `text` and return a list of strings, each containing a WORD'''\n",
      "    # FILL IN CODE\n",
      "\n",
      "    return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test your function by running this cell\n",
      "\n",
      "if tokenizer(easy_text) == easy_words:\n",
      "    print 'Congratulations!'\n",
      "else:\n",
      "    print 'Sorry, try again!'\n",
      "    print\n",
      "    print 'Your version:'\n",
      "    print tokenizer(easy_text)\n",
      "    print\n",
      "    print 'Desired output:'\n",
      "    print easy_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test your function by running this cell\n",
      "\n",
      "if tokenizer(med_text) == med_words:\n",
      "    print 'Congratulations!'\n",
      "else:\n",
      "    print 'Sorry, try again!'\n",
      "    print\n",
      "    print 'Your version:'\n",
      "    print tokenizer(med_text)\n",
      "    print\n",
      "    print 'Desired output:'\n",
      "    print med_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test your function by running this cell\n",
      "\n",
      "if tokenizer(hard_text) == hard_words:\n",
      "    print 'Congratulations!'\n",
      "else:\n",
      "    print 'Sorry, try again!'\n",
      "    print\n",
      "    print 'Your version:'\n",
      "    print tokenizer(hard_text)\n",
      "    print\n",
      "    print 'Desired output:'\n",
      "    print hard_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Tokenization continued\n",
      "\n",
      "Let's see how NLTK [tokenizes text into words](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import word_tokenize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print word_tokenize(easy_text)\n",
      "print word_tokenize(med_text)\n",
      "print word_tokenize(hard_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It behaves a little differently, and sometimes erratically. Let's try a version based on pattern-matching."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import wordpunct_tokenize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print wordpunct_tokenize(easy_text)\n",
      "print wordpunct_tokenize(med_text)\n",
      "print wordpunct_tokenize(hard_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the end, there isn't really a right or wrong way to tokenize words. Sometimes punctuation provides valuable semantic content. Sometimes, you want to strip it all away.\n",
      "\n",
      "As a final thought, what do you suppose the following functions do? Go ahead and play with them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import bigrams, trigrams, ngrams"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print bigrams(word_tokenize(easy_text))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "HTML('''\n",
      "<style>\n",
      ".text_cell_render {\n",
      "  background-color: cyan;\n",
      "}\n",
      "</style>\n",
      "''')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}