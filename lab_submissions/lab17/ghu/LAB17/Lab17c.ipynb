{
 "metadata": {
  "name": "",
  "signature": "sha256:76d80fd7a8d3d79cd5065b276d80cf7c30f93c438a4fc3a3eb93d7ec30f54ded"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# stop words, tf-idf\n",
      "\n",
      "Let's investigate one of the most useful feature weightings, and how stop words derive naturally from that. To start, let's load a set of small documents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load data\n",
      "# the try block should work from your lab submission folder. The except block works from the lesson folder.\n",
      "\n",
      "try:\n",
      "    df = pd.read_csv('../../../lessons/lesson09_probability_naive_bayes/rt_critics.csv')\n",
      "except IOError:\n",
      "    df = pd.read_csv('../lesson09_probability_naive_bayes/rt_critics.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# It seems silly to call such short blurbs 'documents', but we'll stick with the NLP nomenclature.\n",
      "\n",
      "documents = list(df['quote'])\n",
      "documents[:1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "['So ingenious in concept, design and execution that you could watch it on a postage stamp-sized screen and still be engulfed by its charm.']"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Document Frequency\n",
      "\n",
      "Let's start by calculating the document frequency for words in these documents. For this task, let's also remove all the punctuation marks and make everything lower-case."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import wordpunct_tokenize  # for tokenizing our text\n",
      "import string  # helps with removing punctuation\n",
      "from collections import Counter  # great dict-like datastructure for counting things"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This is a bit of text cleanup\n",
      "word_bag_list = []\n",
      "for doc in documents:\n",
      "    cleaned = doc.replace('-', ' ')  # make lowercase and split hyphenated words in two\n",
      "    for c in string.punctuation:  # strip punctuation marks.\n",
      "        cleaned = cleaned.replace(c, '')\n",
      "    word_bag_list.append(wordpunct_tokenize(cleaned))\n",
      "\n",
      "# How do things look?\n",
      "print 'a few tokens:', word_bag_list[:0]\n",
      "\n",
      "# this flattens the nested lists into one big list for some stats\n",
      "token_list = []\n",
      "for tokens in word_bag_list:\n",
      "    token_list.extend(tokens)\n",
      "print 'number of tokens:', len(token_list)\n",
      "print 'number of unique tokens:', len(set(token_list))\n",
      "print 'number of documents:', len(word_bag_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "a few tokens: []\n",
        "number of tokens: 280092\n",
        "number of unique tokens: 25183\n",
        "number of documents: 14072\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculate the document frequency of all the unique tokens in the bags of words.\n",
      "\n",
      "df = Counter()  # initialize this dict-like thing.\n",
      "\n",
      "for doc in word_bag_list:\n",
      "    # FILL IN CODE\n",
      "    # count up the times words appear in INDIVIDUAL documents (not the total across all documents)\n",
      "    df[word_bag_list]\n",
      "    for something in something_else:  # edit this, obviously\n",
      "        # add one to the right key in df\n",
      "        df[word_bag_list].append[:-1]\n",
      "for token in df:\n",
      "    # normalize the counts by the number of documents (are you getting zeros? Think datatypes.)\n",
      "    \n",
      "# this last line prints the 20 highest-scoring words and their scores\n",
      "    df.most_common(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "unhashable type: 'list'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-28-66e2fb033e1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# FILL IN CODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# count up the times words appear in INDIVIDUAL documents (not the total across all documents)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_bag_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msomething\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msomething_else\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# edit this, obviously\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# add one to the right key in df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Stop Words\n",
      "\n",
      "Which words are likely to be stop words? The ones that show up in the most documents! These terms with the largest document frequency are the stopwords! The threshold above which you call something a stopword is up to you."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## tf-idf\n",
      "\n",
      "More interesting than stop-words is the tf-idf score. This tells us which words are most discriminative between documents. Words that occur a lot in one document but doesn't occur in many documents will tell you something special about the document:\n",
      "\n",
      "$$\n",
      "\\text{tf-idf} = tf \\cdot \\log{idf} = tf \\cdot \\log{1 \\over df} = tf \\cdot -\\log{df}\n",
      "$$\n",
      "\n",
      "recall that:\n",
      "\n",
      "$$\n",
      "\\log{x} = -\\log{1 \\over x}\n",
      "$$\n",
      "\n",
      "What are the most discriminative words in the first few documents?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculate the term frequency of all the unique tokens in all of the bags of words.\n",
      "\n",
      "for doc in word_bag_list[:100]:\n",
      "    tf = Counter()  # initialize this dict-like thing.\n",
      "    tfidf = Counter()\n",
      "    \n",
      "    # FILL IN CODE\n",
      "\n",
      "    # calculate term frequencies\n",
      "    # this is similar to the document frequencies.\n",
      "    for something in something_else:\n",
      "\n",
      "    # calculate tf-idf scores\n",
      "    for token in tf:\n",
      "        tfidf[token] = # fill this in. you can use np.log().\n",
      "\n",
      "    # this prints most significant words in the document\n",
      "    print tfidf.most_common(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndentationError",
       "evalue": "expected an indented block (<ipython-input-29-ea05b662a227>, line 14)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-ea05b662a227>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    for token in tf:\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Sci-Kit Learn\n",
      "\n",
      "Scikit-Learn comes with utilities to do these calculations for us. How do their results compare?\n",
      "\n",
      "I confess, I ran out of time to do a proper comparison, but with enough work, we can figure out which features (i.e. words) have the highest scores. What's happening is each documents is converted into a normalized vector (length = 1) where most of the dimensions/features/words are 0, and the words that occur in the document get a score proportional to its tf-idf score."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf_vec = TfidfVectorizer()\n",
      "output = tfidf_vec.fit_transform(documents)\n",
      "print output.toarray()[20:30, :10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.          0.          0.          0.          0.          0.          0.\n",
        "   0.          0.          0.        ]\n",
        " [ 0.          0.          0.          0.          0.          0.          0.\n",
        "   0.          0.          0.        ]\n",
        " [ 0.          0.          0.          0.          0.          0.          0.\n",
        "   0.          0.          0.        ]\n",
        " [ 0.          0.          0.          0.          0.          0.          0.\n",
        "   0.          0.          0.        ]\n",
        " [ 0.          0.          0.          0.          0.25400101  0.          0.\n",
        "   0.          0.          0.        ]\n",
        " [ 0.          0.          0.          0.          0.          0.          0.\n",
        "   0.          0.          0.        ]\n",
        " [ 0.          0.          0.          0.          0.          0.          0.\n",
        "   0.          0.          0.        ]\n",
        " [ 0.          0.          0.          0.          0.          0.          0.\n",
        "   0.          0.          0.        ]\n",
        " [ 0.          0.          0.          0.          0.          0.          0.\n",
        "   0.          0.          0.        ]\n",
        " [ 0.          0.          0.          0.          0.          0.          0.\n",
        "   0.          0.          0.        ]]\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tfidf_vec.get_stop_words()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "None\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "HTML('''\n",
      "<style>\n",
      ".text_cell_render {\n",
      "  background-color: cyan;\n",
      "}\n",
      "</style>\n",
      "''')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "\n",
        "<style>\n",
        ".text_cell_render {\n",
        "  background-color: cyan;\n",
        "}\n",
        "</style>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "<IPython.core.display.HTML at 0x10b4a1fd0>"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}